{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juan-Draghi/relevamiento-boletin-oficial-caba/blob/main/Busqueda_Boletin_Oficial_CABA_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BFr88k05vMH"
      },
      "source": [
        "# üìÑ An√°lisis del Bolet√≠n Oficial de CABA\n",
        "Este notebook procesa **PDF(s) del Bolet√≠n Oficial de la Ciudad de Buenos Aires** que vos subas manualmente.\n",
        "Realiza una **b√∫squeda por t√©rminos clave y patrones de pertinencia (verbos de acci√≥n normativa)** y genera un **Excel** con:\n",
        "1) **Pertinentes (keywords+patrones)**: p√°ginas donde coexisten *al menos un keyword* y *al menos un patr√≥n de pertinencia*.\n",
        "2) **Todos los hallazgos**: p√°ginas donde aparece *alg√∫n keyword* **o** *alg√∫n patr√≥n de pertinencia*."
      ],
      "id": "6BFr88k05vMH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu7ZYnW-5vMJ"
      },
      "source": [
        "## üîß Instalaci√≥n de dependencias"
      ],
      "id": "fu7ZYnW-5vMJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install"
      },
      "source": [
        "!pip install -qq pdfplumber openpyxl tqdm python-dotenv"
      ],
      "id": "install",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG_ttEB-5vML"
      },
      "source": [
        "## üì¶ Importaciones y configuraci√≥n"
      ],
      "id": "hG_ttEB-5vML"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import zlib\n",
        "import hashlib\n",
        "import unicodedata\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "\n",
        "def normalize_spaces(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    \"\"\"Normaliza ligaduras, elimina guiones blandos/espacios invisibles y des-hifena cortes de l√≠nea.\"\"\"\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    # 1) Normaliza Unicode (ej: Ô¨Å -> fi)\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    # 2) Limpia caracteres problem√°ticos\n",
        "    s = s.replace(\"\\x00\", \" \").replace(\"\\xa0\", \" \").replace(\"\\ufeff\", \"\")\n",
        "    s = s.replace(\"\\u00ad\", \"\")  # soft hyphen\n",
        "    s = re.sub(r\"[\\u200B-\\u200D\\u2060]\", \"\", s)  # zero-width\n",
        "    # 3) Une palabras cortadas por gui√≥n de fin de l√≠nea: \"Fi-\\njar\" -> \"Fijar\"\n",
        "    s = re.sub(r\"(\\w)-\\s+(\\w)\", r\"\\1\\2\", s)\n",
        "    # 4) Normaliza saltos y espacios\n",
        "    s = s.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
        "    s = normalize_spaces(s)\n",
        "    return s\n",
        "\n",
        "# Utilidad para extraer un recorte de texto alrededor de una coincidencia\n",
        "def extract_snippet(text, start_idx, window=220):\n",
        "    a = max(0, start_idx - window)\n",
        "    b = min(len(text), start_idx + window)\n",
        "    return text[a:b].strip()"
      ],
      "id": "imports",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl6Acofg5vMM"
      },
      "source": [
        "## ‚¨ÜÔ∏è Sub√≠ el/los PDF(s) del Bolet√≠n Oficial"
      ],
      "id": "cl6Acofg5vMM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upload"
      },
      "source": [
        "# En Colab, ejecut√° esto y seleccion√° 1 o m√°s PDF del Bolet√≠n Oficial.\n",
        "try:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    pdf_paths = [fn for fn in uploaded.keys() if fn.lower().endswith(\".pdf\")]\n",
        "    if not pdf_paths:\n",
        "        raise ValueError(\"No se subieron archivos PDF.\")\n",
        "except Exception as e:\n",
        "    # Si no est√°s en Colab y corr√©s localmente, pod√©s setear manualmente pdf_paths\n",
        "    print(\"Advertencia:\", e)\n",
        "    # Ejemplo para uso local: coloc√° aqu√≠ rutas locales a PDFs si no us√°s Colab\n",
        "    pdf_paths = []\n",
        "    # pdf_paths = [\"/path/a/tu/boletin.pdf\"]"
      ],
      "id": "upload",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh4L_BTL5vMM"
      },
      "source": [
        "## üîé Listas de b√∫squeda: keywords y patrones de pertinencia"
      ],
      "id": "oh4L_BTL5vMM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "terms"
      },
      "source": [
        "keywords = [\n",
        "    \"C√≥digo Urban√≠stico\",\n",
        "    \"C√≥digo de Edificaci√≥n\",\n",
        "    \"C√≥digo de Habilitaciones\",\n",
        "    \"compendio normativo\",\n",
        "    \"Reglamentos T√©cnicos\",\n",
        "    \"Reglamento T√©cnico\",\n",
        "    \"Autorizaci√≥n de actividad econ√≥mica\",\n",
        "    \"Autorizaci√≥n de actividades econ√≥micas\",\n",
        "    \"Autorizaciones de actividades econ√≥micas\",\n",
        "    \"Impacto ambiental\",\n",
        "    \"Ley Marco de Regulaci√≥n de Actividades Econ√≥micas de la Ciudad Aut√≥noma de Buenos Aires\",\n",
        "    \"Ley Tarifaria\",\n",
        "    \"publicidad exterior\",\n",
        "    \"Unidad Tarifaria\",\n",
        "    \"Sistema de Autoprotecci√≥n\",\n",
        "    \"Sistemas de Autoprotecci√≥n\",\n",
        "    \"Catastro\",\n",
        "    \"Derecho para el Desarrollo Urbano y el H√°bitat Sustentable\",\n",
        "    \"C√≥digo Fiscal\",\n",
        "    \"√Årea C√©ntrica\",\n",
        "    \"planos de mensura\",\n",
        "    \"obras en contravenci√≥n\",\n",
        "    \"UERESGP\",\n",
        "    r\"[Dd]isposici[o√≥]n(?: [Nn]¬∞?)? ?3\\\\.?500(?:[-/]?GCABA)?[-/]?DGOEP[-/]?16\",\n",
        "    r\"[Dd]isposici[o√≥]n(?: [Nn]¬∞?)? ?331(?:[-/]?GCABA)?[-/]?DGDCIV[-/]?25\",\n",
        "    r\"[Dd]isposici[o√≥]n(?: [Nn]¬∞?)? ?89(?:[-/]?GCABA)?[-/]?DGROC[-/]?24\",\n",
        "    r\"[Dd]isposici[o√≥]n(?: [Nn]¬∞?)? ?526(?:[-/]?GCABA)?[-/]?DGFYCO[-/]?24\",\n",
        "    r\"[Rr]esoluci[o√≥]n(?: [Nn]¬∞?)? ?275(?:[-/]?GCABA)?[-/]?APRA[-/]?23\",\n",
        "    r\"[Rr]esoluci[o√≥]n(?: [Nn]¬∞?)? ?188(?:[-/]?GCABA)?[-/]?SSGU[-/]?24\",\n",
        "    r\"[Rr]esoluci[o√≥]n(?: [Nn]¬∞?)? ?160(?:[-/]?GCABA)?[-/]?SSHA[-/]?24\",\n",
        "    r\"[Rr]esoluci[o√≥]n(?: [Nn]¬∞?)? ?96(?:[-/]?GCABA)?[-/]?AGC[-/]?25\",\n",
        "    r\"[Rr]esoluci[o√≥]n(?: [Nn]¬∞?)? ?345(?:[-/]?GCABA)?[-/]?AGC[-/]?21\",\n",
        "    r\"[Rr]esoluci[o√≥]n(?: [Nn]¬∞?)? ?103(?:[-/]?GCABA)?[-/]?APRA[-/]?25\",\n",
        "    r\"[Rr]esoluci[o√≥]n(?: [Nn]¬∞?)? ?1(?:[-/]?GCABA)?[-/]?MEPHUGC[-/]?25\",\n",
        "    r\"[Dd]ecreto(?: [Nn]¬∞?)? ?51/18\",\n",
        "    r\"[Dd]ecreto(?: [Nn]¬∞?)? ?86/19\",\n",
        "    r\"[Dd]ecreto(?: [Nn]¬∞?)? ?87/19\",\n",
        "    r\"[Dd]ecreto(?: [Nn]¬∞?)? ?99/19\",\n",
        "    r\"[Dd]ecreto(?: [Nn]¬∞?)? ?105/19\",\n",
        "    r\"[Dd]ecreto(?: [Nn]¬∞?)? ?475/20\",\n",
        "    r\"[Dd]ecreto(?: [Nn]¬∞?)? ?129/25\",\n",
        "    r\"[Dd]decreto(?: [Nn]¬∞?)? ?116/25\",\n",
        "    r\"[Dd]ecreto(?: [Nn]¬∞?)? ?164/25\",\n",
        "    r\"[Dd]ecreto(?: [Nn]¬∞?)? ?189/25\",\n",
        "    r\"[Ll]ey(?: [Nn]¬∞?)? ?123\",\n",
        "    r\"[Ll]ey(?: [Nn]¬∞?)? ?2\\\\.?936\",\n",
        "    r\"[Ll]ey(?: [Nn]¬∞?)? ?5\\\\.?920\",\n",
        "    r\"[Ll]ey(?: [Nn]¬∞?)? ?6\\\\.101\",\n",
        "    r\"[Ll]ey(?: [Nn]¬∞?)? ?6\\\\.?776\",\n",
        "    r\"[Ll]ey(?: [Nn]¬∞?)? ?6\\\\.?779\",\n",
        "    r\"[Ll]ey(?: [Nn]¬∞?)? ?6\\\\.?099\",\n",
        "    r\"[Ll]ey(?: [Nn]¬∞?)? ?6\\\\.?100\",\n",
        "]\n",
        "\n",
        "patrones_pertinencia = [\n",
        "    # Modificar\n",
        "    r\"\\bmodifica\\b\", r\"\\bmodificar\\b\", r\"\\bmodif√≠case\\b\",\n",
        "    # Derogar\n",
        "    r\"\\bderoga\\b\", r\"\\bderogar\\b\", r\"\\bder√≥gase\\b\",\n",
        "    # Aprobar (ambas f√≥rmulas)\n",
        "    r\"\\baprueba\\b\", r\"\\baprobar\\b\", r\"\\bapru√©bese\\b\", r\"\\bapru√©base\\b\",\n",
        "    # Dejar sin efecto\n",
        "    r\"\\bdeja sin efecto\\b\", r\"\\bdejar sin efecto\\b\", r\"\\bd√©jase sin efecto\\b\",\n",
        "    # Sustituir\n",
        "    r\"\\bsustituye\\b\", r\"\\bsustituir\\b\", r\"\\bsustit√∫yese\\b\", r\"\\bsustit√∫yase\\b\",\n",
        "    # Establecer\n",
        "    r\"\\bestablece\\b\", r\"\\bestablecer\\b\", r\"\\bestabl√©cese\\b\", r\"\\bestabl√©case\\b\",\n",
        "    # Fijar (agrego la f√≥rmula ‚ÄúF√≠jase‚Äù)\n",
        "    r\"\\bfija\\b\", r\"\\bfijar\\b\", r\"\\bf√≠jese\\b\", r\"\\bf√≠jase\\b\",\n",
        "    # Determinar\n",
        "    r\"\\bdetermina\\b\", r\"\\bdeterminar\\b\", r\"\\bdeterm√≠nase\\b\", r\"\\bdeterm√≠nese\\b\",\n",
        "    # Reglamentar (ambas)\n",
        "    r\"\\breglamenta\\b\", r\"\\breglamentar\\b\", r\"\\breglam√©ntese\\b\", r\"\\breglam√©ntase\\b\", r\"\\breglamentaci√≥n\\b\",\n",
        "    # Prorrogar\n",
        "    r\"\\bprorroga\\b\", r\"\\bprorrogar\\b\", r\"\\bprorr√≥gase\\b\", r\"\\bprorr√≥gese\\b\",\n",
        "    # Incorporar / Crear (muy frecuentes en anexos)\n",
        "    r\"\\bincorpora\\b\", r\"\\bincorporar\\b\", r\"\\bincorp√≥rase\\b\", r\"\\bincorp√≥rese\\b\",\n",
        "    r\"\\bcrea\\b\", r\"\\bcrear\\b\", r\"\\bcr√©ase\\b\", r\"\\bcr√©ese\\b\",\n",
        "    # Otras f√≥rmulas habituales\n",
        "    r\"\\bdeclara\\b\", r\"\\bdeclarar\\b\", r\"\\bdecl√°rase\\b\", r\"\\bdecl√°rese\\b\",\n",
        "    r\"\\botorga\\b\", r\"\\botorgar\\b\", r\"\\bot√≥rgase\\b\", r\"\\bot√≥rguese\\b\",\n",
        "    r\"\\brectifica\\b\", r\"\\brectificar\\b\", r\"\\brectif√≠case\\b\", r\"\\brectif√≠quese\\b\",\n",
        "]\n",
        "patrones_pertinencia_comp = [re.compile(p, re.IGNORECASE) for p in patrones_pertinencia]\n",
        "\n",
        "# Separa keywords en \"simples\" (texto literal) y \"regulares\" (contienen clases/escapes t√≠picos)\n",
        "def es_patron_regex(s: str) -> bool:\n",
        "    # Heur√≠stica m√≠nima: si contiene metacaracteres frecuentes, lo tratamos como regex\n",
        "    return bool(re.search(r\"[\\\\\\[\\]\\(\\)\\?\\+\\*\\|]\", s))\n",
        "\n",
        "keywords_regex = [k for k in keywords if es_patron_regex(k)]\n",
        "keywords_simples = [k for k in keywords if not es_patron_regex(k)]\n",
        "\n",
        "# Compilamos patrones de pertinencia y keywords regex para acelerar\n",
        "patrones_pertinencia_comp = [re.compile(p, re.IGNORECASE) for p in patrones_pertinencia]\n",
        "keywords_regex_comp = [re.compile(p, re.IGNORECASE) for p in keywords_regex]"
      ],
      "id": "terms",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O0TX2DJ5vMN"
      },
      "source": [
        "## ‚ñ∂Ô∏è Procesamiento y exportaci√≥n"
      ],
      "id": "5O0TX2DJ5vMN"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- CONFIG ----------\n",
        "WINDOW_WORDS = 300          # ¬± palabras alrededor del keyword\n",
        "MERGE_GAP_WORDS = 100       # unir contextos si est√°n a <100 palabras\n",
        "SIMHASH_BITS = 64           # tama√±o de simhash\n",
        "SIMHASH_THRESH = 3          # distancia de Hamming m√°xima para considerar duplicado\n",
        "CACHE_DIR = \"/content/bo_cache\"  # cambia si quer√©s otro path\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- NORMALIZACI√ìN (asegurate de tener estas utilidades definidas) ----------\n",
        "def normalize_spaces(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    if not s: return \"\"\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = (s.replace(\"\\x00\",\" \").replace(\"\\xa0\",\" \").replace(\"\\ufeff\",\"\")\n",
        "           .replace(\"\\u00ad\",\"\"))  # soft hyphen\n",
        "    s = re.sub(r\"[\\u200B-\\u200D\\u2060]\", \"\", s)  # zero-width\n",
        "    # Une cortes \"Fi-\\njar\" -> \"Fijar\"\n",
        "    s = re.sub(r\"(\\w)-\\s+(\\w)\", r\"\\1\\2\", s)\n",
        "    s = s.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
        "    # normaliza guiones raros a \"-\"\n",
        "    s = s.translate(str.maketrans({c:\"-\" for c in \"‚Äê-‚Äí‚Äì‚Äî\"}))\n",
        "    return normalize_spaces(s)\n",
        "\n",
        "# ---------- TOKENIZACI√ìN ----------\n",
        "def tokenize_with_spans(text):\n",
        "    toks = []\n",
        "    for m in re.finditer(r\"\\S+\", text):\n",
        "        toks.append((m.group(0), m.start(), m.end()))\n",
        "    return toks\n",
        "\n",
        "def charpos_to_word_index(spans, pos):\n",
        "    # b√∫squeda lineal (robusta y suficiente)\n",
        "    for i, (_, s, e) in enumerate(spans):\n",
        "        if s <= pos < e:\n",
        "            return i\n",
        "    if not spans:\n",
        "        return 0\n",
        "    return min(range(len(spans)), key=lambda i: abs(spans[i][1]-pos))\n",
        "\n",
        "# ---------- SIMHASH DEDUP ----------\n",
        "def _hash64(x: str) -> int:\n",
        "    # hash determinista 64-bit\n",
        "    return int(hashlib.blake2b(x.encode(\"utf-8\"), digest_size=8).hexdigest(), 16)\n",
        "\n",
        "def simhash(text: str, bits: int = SIMHASH_BITS) -> int:\n",
        "    # tokens simples (palabras); pod√©s cambiar a shingles si quer√©s m√°s fineza\n",
        "    tokens = re.findall(r\"\\w{3,}\", text.lower())\n",
        "    v = [0]*bits\n",
        "    for t in tokens:\n",
        "        h = _hash64(t)\n",
        "        for b in range(bits):\n",
        "            bit = (h >> b) & 1\n",
        "            v[b] += 1 if bit else -1\n",
        "    out = 0\n",
        "    for b in range(bits):\n",
        "        if v[b] >= 0:\n",
        "            out |= (1 << b)\n",
        "    return out\n",
        "\n",
        "def hamming(a: int, b: int) -> int:\n",
        "    return (a ^ b).bit_count()\n",
        "\n",
        "def dedup_by_simhash(rows, text_key=\"Extracto\", keep=\"max_score\"):\n",
        "    \"\"\"Dedup por SimHash. keep: 'max_score' o 'first'.\"\"\"\n",
        "    kept = []\n",
        "    signatures = []  # (simhash, idx_en_kept)\n",
        "    for r in rows:\n",
        "        text = r.get(text_key, \"\") or \"\"\n",
        "        sh = simhash(text)\n",
        "        dup_idx = None\n",
        "        for j, (sh2, kidx) in enumerate(signatures):\n",
        "            if hamming(sh, sh2) <= SIMHASH_THRESH:\n",
        "                dup_idx = kidx\n",
        "                break\n",
        "        if dup_idx is None:\n",
        "            kept.append(r)\n",
        "            signatures.append((sh, len(kept)-1))\n",
        "        else:\n",
        "            if keep == \"max_score\":\n",
        "                # si el nuevo tiene score mayor, reemplaza\n",
        "                if r.get(\"Score\", 0) > kept[dup_idx].get(\"Score\", 0):\n",
        "                    kept[dup_idx] = r\n",
        "                    signatures[dup_idx] = (sh, dup_idx)\n",
        "    return kept\n",
        "\n",
        "# ---------- COMPILACI√ìN REGEX (UNA VEZ) ----------\n",
        "patrones_pertinencia_comp = [re.compile(p, re.IGNORECASE) for p in patrones_pertinencia]\n",
        "\n",
        "def es_patron_regex(s: str) -> bool:\n",
        "    return bool(re.search(r\"[\\\\\\[\\]\\(\\)\\?\\+\\*\\|]\", s))\n",
        "\n",
        "keywords_regex = [k for k in keywords if es_patron_regex(k)]\n",
        "keywords_simples = [k for k in keywords if not es_patron_regex(k)]\n",
        "keywords_regex_comp = [re.compile(p, re.IGNORECASE) for p in keywords_regex]\n",
        "keywords_simples_lower = [k.lower() for k in keywords_simples]  # para contains r√°pido\n",
        "\n",
        "# ---------- CACHE ----------\n",
        "def pdf_sha256(path: str) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1<<20), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def cache_path(pdf_hash: str) -> str:\n",
        "    return os.path.join(CACHE_DIR, f\"{pdf_hash}.json\")\n",
        "\n",
        "def load_cache(pdf_hash: str):\n",
        "    p = cache_path(pdf_hash)\n",
        "    if os.path.exists(p):\n",
        "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    return None\n",
        "\n",
        "def save_cache(pdf_hash: str, data):\n",
        "    with open(cache_path(pdf_hash), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False)\n",
        "\n",
        "# ---------- PASADA R√ÅPIDA (marca ventanas candidatas por p√°gina) ----------\n",
        "def quick_scan_page(args):\n",
        "    pdf_path, page_index = args\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            page = pdf.pages[page_index]\n",
        "            raw = page.extract_text() or \"\"\n",
        "        text = clean_text(raw)\n",
        "        # pistas m√≠nimas por performance\n",
        "        lower_text = text.lower()\n",
        "        hit_positions = []\n",
        "\n",
        "        # simples (contains)\n",
        "        for kw, q in zip(keywords_simples, keywords_simples_lower):\n",
        "            start = 0\n",
        "            while True:\n",
        "                idx = lower_text.find(q, start)\n",
        "                if idx == -1: break\n",
        "                hit_positions.append((\"simple\", kw, idx, idx+len(kw)))\n",
        "                start = idx + max(1, len(q)//2)\n",
        "\n",
        "        # regex\n",
        "        for rx in keywords_regex_comp:\n",
        "            for m in rx.finditer(text):\n",
        "                hit_positions.append((\"regex\", rx.pattern, m.start(), m.end()))\n",
        "\n",
        "        return {\n",
        "            \"ok\": True,\n",
        "            \"page_index\": page_index,\n",
        "            \"text\": text,\n",
        "            \"hits\": hit_positions\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"ok\": False, \"page_index\": page_index, \"error\": str(e)}\n",
        "\n",
        "# ---------- MERGE DE CONTEXTOS SOLAPADOS ----------\n",
        "def merge_contexts(spans, tokens, hit_positions, window_words=WINDOW_WORDS, gap=MERGE_GAP_WORDS):\n",
        "    \"\"\"Crea ventanas ¬±window_words por cada hit y las une si est√°n a <gap palabras.\"\"\"\n",
        "    # construir spans de palabra\n",
        "    windows = []\n",
        "    for (_, kw, start, _end) in hit_positions:\n",
        "        widx = charpos_to_word_index(spans, start)\n",
        "        a = max(0, widx - window_words)\n",
        "        b = min(len(tokens), widx + window_words + 1)\n",
        "        windows.append({\"a\": a, \"b\": b, \"keywords\": set([kw])})\n",
        "\n",
        "    if not windows:\n",
        "        return []\n",
        "\n",
        "    # ordenar por inicio y merge\n",
        "    windows.sort(key=lambda x: x[\"a\"])\n",
        "    merged = [windows[0]]\n",
        "    for w in windows[1:]:\n",
        "        last = merged[-1]\n",
        "        if w[\"a\"] <= last[\"b\"] + gap:\n",
        "            # solapa o cerca: unir\n",
        "            last[\"b\"] = max(last[\"b\"], w[\"b\"])\n",
        "            last[\"keywords\"].update(w[\"keywords\"])\n",
        "        else:\n",
        "            merged.append(w)\n",
        "    # construir contextos\n",
        "    out = []\n",
        "    for w in merged:\n",
        "        contexto = \" \".join(tokens[w[\"a\"]:w[\"b\"]])\n",
        "        out.append({\"a\": w[\"a\"], \"b\": w[\"b\"], \"keywords\": sorted(list(w[\"keywords\"])), \"context\": contexto})\n",
        "    return out\n",
        "\n",
        "# ---------- PATRONES EN CONTEXTO + SCORE ----------\n",
        "VERBOS_FUERTES = re.compile(r\"(aprueba|deroga|establece|fija|apruebese|apruebase|derogase|establ√©cese|f√≠jase|f√≠jese)\", re.IGNORECASE)\n",
        "\n",
        "def patrones_en_contexto(ctx_text):\n",
        "    hits = []\n",
        "    for prx in patrones_pertinencia_comp:\n",
        "        if prx.search(ctx_text):\n",
        "            hits.append(prx.pattern)\n",
        "    return hits\n",
        "\n",
        "def score_context(ctx_text, pats):\n",
        "    score = 0\n",
        "    if any(VERBOS_FUERTES.search(ctx_text) for _ in [0]):  # r√°pido\n",
        "        score += 2\n",
        "    if pats:\n",
        "        score += 1\n",
        "    # ‚ÄúArt√≠culo 1¬∞/Primero/RESUELVE‚Äù suma\n",
        "    if re.search(r\"Art√≠culo\\s*1|Art\\.\\s*1|RESUELVE|DISP√ìNESE|EL JEFE DE GOBIERNO RESUELVE\", ctx_text, re.IGNORECASE):\n",
        "        score += 1\n",
        "    return score\n",
        "\n",
        "# ---------- PASADA DETALLE (sobre ventanas candidatas) ----------\n",
        "def detail_scan_page(pdf_path, page_index, text, hits):\n",
        "    spans = tokenize_with_spans(text)\n",
        "    tokens = [t for (t, s, e) in spans]\n",
        "    contexts = merge_contexts(spans, tokens, hits, WINDOW_WORDS, MERGE_GAP_WORDS)\n",
        "    rows_all = []\n",
        "    rows_pert = []\n",
        "    for c in contexts:\n",
        "        pats = patrones_en_contexto(c[\"context\"])\n",
        "        sc = score_context(c[\"context\"], pats)\n",
        "        row = {\n",
        "            \"Archivo\": os.path.basename(pdf_path),\n",
        "            \"P√°gina\": page_index + 1,\n",
        "            \"Keywords_en_contexto\": \"; \".join(c[\"keywords\"]),\n",
        "            \"Coincidencias_patrones_en_contexto\": \"; \".join(pats),\n",
        "            \"Hay_patron_en_contexto\": bool(pats),\n",
        "            \"Score\": sc,\n",
        "            \"Extracto\": c[\"context\"]\n",
        "        }\n",
        "        rows_all.append(row)\n",
        "        if pats:\n",
        "            rows_pert.append(row)\n",
        "    return rows_all, rows_pert\n",
        "\n",
        "# =========================\n",
        "#     EJECUCI√ìN\n",
        "# =========================\n",
        "all_rows = []\n",
        "pertinent_rows = []\n",
        "\n",
        "for pdf_path in pdf_paths:\n",
        "    pdf_hash = pdf_sha256(pdf_path)\n",
        "    cache = load_cache(pdf_hash)\n",
        "\n",
        "    # Si no hay cache, o est√° incompleta, la regeneramos\n",
        "    if not cache:\n",
        "        # PASADA R√ÅPIDA EN PARALELO (por p√°gina)\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            n_pages = len(pdf.pages)\n",
        "        tasks = [(pdf_path, i) for i in range(n_pages)]\n",
        "        results = [None]*len(tasks)\n",
        "        with ProcessPoolExecutor() as ex:\n",
        "            futures = {ex.submit(quick_scan_page, t): t for t in tasks}\n",
        "            for fut in tqdm(as_completed(futures), total=len(tasks), desc=f\"Quick-scan {os.path.basename(pdf_path)}\"):\n",
        "                res = fut.result()\n",
        "                results[res[\"page_index\"]] = res\n",
        "        # guardamos cache m√≠nima: texto normalizado + hits por p√°gina\n",
        "        cache = {\"pages\": []}\n",
        "        for r in results:\n",
        "            if not r or not r.get(\"ok\"):\n",
        "                cache[\"pages\"].append({\"ok\": False, \"error\": r.get(\"error\",\"\") if r else \"unknown\"})\n",
        "            else:\n",
        "                cache[\"pages\"].append({\n",
        "                    \"ok\": True,\n",
        "                    \"text\": r[\"text\"],\n",
        "                    \"hits\": r[\"hits\"]  # lista de (tipo, kw/patr√≥n, start, end)\n",
        "                })\n",
        "        save_cache(pdf_hash, cache)\n",
        "\n",
        "    # PASADA DETALLE SOLO EN P√ÅGINAS CANDIDATAS (con hits)\n",
        "    for idx, pg in enumerate(cache[\"pages\"]):\n",
        "        if not pg.get(\"ok\"):\n",
        "            continue\n",
        "        text = pg.get(\"text\",\"\")\n",
        "        hits = pg.get(\"hits\", [])\n",
        "        if not hits:\n",
        "            continue  # sin keywords ‚Üí no detalle\n",
        "        rows_all, rows_pert = detail_scan_page(pdf_path, idx, text, hits)\n",
        "        all_rows.extend(rows_all)\n",
        "        pertinent_rows.extend(rows_pert)\n",
        "\n",
        "# ---------- DEDUP GLOBAL POR SIMHASH ----------\n",
        "all_rows = dedup_by_simhash(all_rows, text_key=\"Extracto\", keep=\"max_score\")\n",
        "pertinent_rows = dedup_by_simhash(pertinent_rows, text_key=\"Extracto\", keep=\"max_score\")\n",
        "\n",
        "# ---------- EXPORTAR EXCEL ----------\n",
        "df_all = pd.DataFrame(all_rows)\n",
        "df_pert = pd.DataFrame(pertinent_rows)\n",
        "\n",
        "fecha_tag = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")\n",
        "nombre_xlsx = f\"Resultados_BO_CABA_sin_LLM_{fecha_tag}.xlsx\"\n",
        "\n",
        "with pd.ExcelWriter(nombre_xlsx, engine=\"openpyxl\") as writer:\n",
        "    (df_pert if not df_pert.empty else pd.DataFrame(\n",
        "        columns=[\"Archivo\",\"P√°gina\",\"Keywords_en_contexto\",\n",
        "                 \"Coincidencias_patrones_en_contexto\",\"Hay_patron_en_contexto\",\"Score\",\"Extracto\"])\n",
        "    ).to_excel(writer, sheet_name=\"Pertinentes (keywords+patrones)\", index=False)\n",
        "\n",
        "    (df_all if not df_all.empty else pd.DataFrame(\n",
        "        columns=[\"Archivo\",\"P√°gina\",\"Keywords_en_contexto\",\n",
        "                 \"Coincidencias_patrones_en_contexto\",\"Hay_patron_en_contexto\",\"Score\",\"Extracto\"])\n",
        "    ).to_excel(writer, sheet_name=\"Todos los hallazgos\", index=False)\n",
        "\n",
        "print(\"Archivo generado:\", nombre_xlsx)\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(nombre_xlsx)\n",
        "except Exception:\n",
        "    pass"
      ],
      "metadata": {
        "id": "bMQtg3hGzq_v"
      },
      "id": "bMQtg3hGzq_v",
      "execution_count": null,
      "outputs": []
    }
  ]
}